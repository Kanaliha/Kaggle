{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training catboost\n",
      "Fold 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score: 0.954227\n",
      "Fold 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score: 0.948810\n",
      "Fold 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score: 0.949394\n",
      "\n",
      "catboost Results:\n",
      "Mean AUC: 0.950810 (±0.002428)\n",
      "\n",
      "Training xgboost\n",
      "Fold 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\urpruj\\AppData\\Local\\miniconda3\\envs\\Kaggle_env\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [06:30:17] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Must have at least 1 validation dataset for early stopping.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 288\u001b[0m\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m submission, importance_summary\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 288\u001b[0m     submission, importance_summary \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 265\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m    264\u001b[0m model \u001b[38;5;241m=\u001b[39m LoanApprovalModel(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m--> 265\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;66;03m# Analyze feature importance\u001b[39;00m\n\u001b[0;32m    268\u001b[0m analyzer \u001b[38;5;241m=\u001b[39m FeatureImportanceAnalyzer()\n",
      "Cell \u001b[1;32mIn[6], line 216\u001b[0m, in \u001b[0;36mLoanApprovalModel.train_and_evaluate\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m    215\u001b[0m fold_model \u001b[38;5;241m=\u001b[39m clone(model)\n\u001b[1;32m--> 216\u001b[0m \u001b[43mfold_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;66;03m# Validate\u001b[39;00m\n\u001b[0;32m    219\u001b[0m val_pred \u001b[38;5;241m=\u001b[39m fold_model\u001b[38;5;241m.\u001b[39mpredict_proba(X_val)[:, \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\urpruj\\AppData\\Local\\miniconda3\\envs\\Kaggle_env\\Lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\urpruj\\AppData\\Local\\miniconda3\\envs\\Kaggle_env\\Lib\\site-packages\\xgboost\\sklearn.py:1531\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[0;32m   1511\u001b[0m model, metric, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(xgb_model, params)\n\u001b[0;32m   1512\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[0;32m   1513\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[0;32m   1514\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1528\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[0;32m   1529\u001b[0m )\n\u001b[1;32m-> 1531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1534\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[0;32m   1546\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\urpruj\\AppData\\Local\\miniconda3\\envs\\Kaggle_env\\Lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\urpruj\\AppData\\Local\\miniconda3\\envs\\Kaggle_env\\Lib\\site-packages\\xgboost\\training.py:182\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    181\u001b[0m     bst\u001b[38;5;241m.\u001b[39mupdate(dtrain, iteration\u001b[38;5;241m=\u001b[39mi, fobj\u001b[38;5;241m=\u001b[39mobj)\n\u001b[1;32m--> 182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcb_container\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mafter_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    183\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    185\u001b[0m bst \u001b[38;5;241m=\u001b[39m cb_container\u001b[38;5;241m.\u001b[39mafter_training(bst)\n",
      "File \u001b[1;32mc:\\Users\\urpruj\\AppData\\Local\\miniconda3\\envs\\Kaggle_env\\Lib\\site-packages\\xgboost\\callback.py:261\u001b[0m, in \u001b[0;36mCallbackContainer.after_iteration\u001b[1;34m(self, model, epoch, dtrain, evals)\u001b[0m\n\u001b[0;32m    259\u001b[0m     metric_score \u001b[38;5;241m=\u001b[39m _parse_eval_str(score)\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_history(metric_score, epoch)\n\u001b[1;32m--> 261\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43many\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mafter_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32mc:\\Users\\urpruj\\AppData\\Local\\miniconda3\\envs\\Kaggle_env\\Lib\\site-packages\\xgboost\\callback.py:261\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    259\u001b[0m     metric_score \u001b[38;5;241m=\u001b[39m _parse_eval_str(score)\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_history(metric_score, epoch)\n\u001b[1;32m--> 261\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mafter_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks)\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32mc:\\Users\\urpruj\\AppData\\Local\\miniconda3\\envs\\Kaggle_env\\Lib\\site-packages\\xgboost\\callback.py:446\u001b[0m, in \u001b[0;36mEarlyStopping.after_iteration\u001b[1;34m(self, model, epoch, evals_log)\u001b[0m\n\u001b[0;32m    444\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust have at least 1 validation dataset for early stopping.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(evals_log\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 446\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m    448\u001b[0m \u001b[38;5;66;03m# Get data name\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata:\n",
      "\u001b[1;31mValueError\u001b[0m: Must have at least 1 validation dataset for early stopping."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Tuple, List, Optional, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, clone\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "@dataclass\n",
    "class FeatureImportance:\n",
    "    \"\"\"Class to store feature importance information.\"\"\"\n",
    "    feature_names: List[str]\n",
    "    importance_values: Dict[str, np.ndarray]\n",
    "    importance_types: Dict[str, str]\n",
    "\n",
    "class DataLoader:\n",
    "    \"\"\"Class to handle data loading and initial preprocessing.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_data() -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.DataFrame]:\n",
    "        \"\"\"Load and combine the training and test datasets.\"\"\"\n",
    "        train = pd.read_csv('train.csv', index_col='id')\n",
    "        test = pd.read_csv('test.csv', index_col='id')\n",
    "        submission = pd.read_csv('sample_submission.csv', index_col='id')\n",
    "        original_data = pd.read_csv('credit_risk_dataset.csv')\n",
    "        \n",
    "        train_df = pd.concat([train, original_data])\n",
    "        X_train = train_df.drop(['loan_status'], axis=1)\n",
    "        y_train = train_df['loan_status']\n",
    "        \n",
    "        return X_train, test, y_train, submission\n",
    "\n",
    "class CustomPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Custom preprocessor that handles both numerical and categorical features.\"\"\"\n",
    "    \n",
    "    def __init__(self, categorical_features: Optional[List[str]] = None):\n",
    "        self.categorical_features = categorical_features\n",
    "        self.numerical_features = None\n",
    "        self.preprocessor = None\n",
    "        self.feature_names = None\n",
    "    \n",
    "    def get_feature_names(self) -> List[str]:\n",
    "        \"\"\"Return list of feature names after transformation.\"\"\"\n",
    "        return self.feature_names\n",
    "    \n",
    "    def fit(self, X: pd.DataFrame, y=None):\n",
    "        \"\"\"Fit the preprocessor on the training data.\"\"\"\n",
    "        if self.categorical_features is None:\n",
    "            self.categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        \n",
    "        self.numerical_features = X.select_dtypes(include=['number']).columns.tolist()\n",
    "        \n",
    "        # Create preprocessing steps for numerical and categorical features\n",
    "        numeric_transformer = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        \n",
    "        categorical_transformer = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "            ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "        ])\n",
    "        \n",
    "        # Combine transformers\n",
    "        self.preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numeric_transformer, self.numerical_features),\n",
    "                ('cat', categorical_transformer, self.categorical_features)\n",
    "            ])\n",
    "        \n",
    "        self.preprocessor.fit(X)\n",
    "        \n",
    "        # Store feature names\n",
    "        self.feature_names = self.numerical_features + self.categorical_features\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Transform the data.\"\"\"\n",
    "        return self.preprocessor.transform(X)\n",
    "\n",
    "class FeatureImportanceAnalyzer:\n",
    "    \"\"\"Class to analyze and visualize feature importances.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_feature_importance(importance: FeatureImportance, top_n: int = 20):\n",
    "        \"\"\"Plot feature importance for each model.\"\"\"\n",
    "        n_models = len(importance.importance_values)\n",
    "        fig, axes = plt.subplots(1, n_models, figsize=(20, 8))\n",
    "        \n",
    "        for idx, (model_name, importance_vals) in enumerate(importance.importance_values.items()):\n",
    "            # Sort features by importance\n",
    "            sorted_idx = np.argsort(importance_vals)\n",
    "            pos = np.arange(sorted_idx[-top_n:].shape[0]) + .5\n",
    "            \n",
    "            # Plot\n",
    "            ax = axes[idx] if n_models > 1 else axes\n",
    "            ax.barh(pos, importance_vals[sorted_idx[-top_n:]])\n",
    "            ax.set_yticks(pos)\n",
    "            ax.set_yticklabels(np.array(importance.feature_names)[sorted_idx[-top_n:]])\n",
    "            ax.set_title(f'{model_name}\\n({importance.importance_types[model_name]})')\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_feature_importance_summary(importance: FeatureImportance) -> pd.DataFrame:\n",
    "        \"\"\"Create a summary DataFrame of feature importances across all models.\"\"\"\n",
    "        summary_dict = {}\n",
    "        \n",
    "        for model_name, imp_values in importance.importance_values.items():\n",
    "            # Normalize importance values\n",
    "            normalized_imp = imp_values / np.sum(imp_values)\n",
    "            summary_dict[f\"{model_name}_importance\"] = normalized_imp\n",
    "        \n",
    "        summary_df = pd.DataFrame(summary_dict, index=importance.feature_names)\n",
    "        \n",
    "        # Add mean importance across all models\n",
    "        summary_df['mean_importance'] = summary_df.mean(axis=1)\n",
    "        summary_df = summary_df.sort_values('mean_importance', ascending=False)\n",
    "        \n",
    "        return summary_df\n",
    "\n",
    "class LoanApprovalModel:\n",
    "    \"\"\"Main model class that handles training and prediction.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_splits: int = 10, random_state: int = 42):\n",
    "        self.n_splits = n_splits\n",
    "        self.random_state = random_state\n",
    "        self.preprocessor = CustomPreprocessor()\n",
    "        self.models = {}\n",
    "        self.fold_scores = {}\n",
    "        self.feature_importance = None\n",
    "    \n",
    "    def _init_models(self):\n",
    "        \"\"\"Initialize the models with default parameters.\"\"\"\n",
    "        self.models = {\n",
    "            'catboost': CatBoostClassifier(\n",
    "                loss_function='Logloss',\n",
    "                eval_metric='AUC',\n",
    "                iterations=5000,\n",
    "                early_stopping_rounds=200,\n",
    "                task_type='GPU',\n",
    "                random_seed=self.random_state,\n",
    "                verbose=False\n",
    "            ),\n",
    "            'xgboost': XGBClassifier(\n",
    "                objective='binary:logistic',\n",
    "                eval_metric='auc',\n",
    "                use_label_encoder=False,\n",
    "                enable_categorical=True,\n",
    "                n_estimators=5000,\n",
    "                early_stopping_rounds=200,\n",
    "                tree_method='hist',\n",
    "                random_state=self.random_state\n",
    "            ),\n",
    "            'lightgbm': LGBMClassifier(\n",
    "                objective='binary',\n",
    "                metric='auc',\n",
    "                n_estimators=5000,\n",
    "                early_stopping_rounds=200,\n",
    "                random_state=self.random_state,\n",
    "                verbose=-1\n",
    "            )\n",
    "        }\n",
    "    \n",
    "    def _get_feature_importance(self, model_name: str, model) -> np.ndarray:\n",
    "        \"\"\"Extract feature importance from a model.\"\"\"\n",
    "        if model_name == 'catboost':\n",
    "            return model.get_feature_importance()\n",
    "        elif model_name == 'xgboost':\n",
    "            return model.feature_importances_\n",
    "        elif model_name == 'lightgbm':\n",
    "            return model.feature_importances_\n",
    "        return np.zeros(len(self.preprocessor.get_feature_names()))\n",
    "    \n",
    "    def train_and_evaluate(self, X: pd.DataFrame, y: pd.Series) -> dict:\n",
    "        \"\"\"Train models using cross-validation and return predictions.\"\"\"\n",
    "        self._init_models()\n",
    "        X_processed = self.preprocessor.fit_transform(X)\n",
    "        \n",
    "        predictions = {}\n",
    "        feature_importances = {model_name: np.zeros(len(self.preprocessor.get_feature_names())) \n",
    "                             for model_name in self.models.keys()}\n",
    "        importance_types = {\n",
    "            'catboost': 'Feature Importance',\n",
    "            'xgboost': 'Gain',\n",
    "            'lightgbm': 'Split'\n",
    "        }\n",
    "        \n",
    "        skf = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=self.random_state)\n",
    "        \n",
    "        for model_name, model in self.models.items():\n",
    "            print(f\"\\nTraining {model_name}\")\n",
    "            fold_predictions = pd.DataFrame()\n",
    "            fold_scores = []\n",
    "            \n",
    "            for fold, (train_idx, val_idx) in enumerate(skf.split(X_processed, y)):\n",
    "                print(f\"Fold {fold + 1}/{self.n_splits}\")\n",
    "                \n",
    "                X_train, X_val = X_processed[train_idx], X_processed[val_idx]\n",
    "                y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "                \n",
    "                # Train model\n",
    "                fold_model = clone(model)\n",
    "                fold_model.fit(X_train, y_train)\n",
    "                \n",
    "                # Validate\n",
    "                val_pred = fold_model.predict_proba(X_val)[:, 1]\n",
    "                fold_score = roc_auc_score(y_val, val_pred)\n",
    "                fold_scores.append(fold_score)\n",
    "                print(f\"AUC score: {fold_score:.6f}\")\n",
    "                \n",
    "                # Store predictions and feature importance\n",
    "                fold_predictions[fold] = fold_model.predict_proba(X_processed)[:, 1]\n",
    "                feature_importances[model_name] += self._get_feature_importance(model_name, fold_model)\n",
    "            \n",
    "            # Average feature importances across folds\n",
    "            feature_importances[model_name] /= self.n_splits\n",
    "            \n",
    "            self.fold_scores[model_name] = fold_scores\n",
    "            predictions[model_name] = fold_predictions\n",
    "            \n",
    "            print(f\"\\n{model_name} Results:\")\n",
    "            print(f\"Mean AUC: {np.mean(fold_scores):.6f} (±{np.std(fold_scores):.6f})\")\n",
    "        \n",
    "        # Store feature importance information\n",
    "        self.feature_importance = FeatureImportance(\n",
    "            feature_names=self.preprocessor.get_feature_names(),\n",
    "            importance_values=feature_importances,\n",
    "            importance_types=importance_types\n",
    "        )\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Generate predictions for test data.\"\"\"\n",
    "        X_processed = self.preprocessor.transform(X)\n",
    "        \n",
    "        final_predictions = []\n",
    "        for model_name, model in self.models.items():\n",
    "            model_preds = model.predict_proba(X_processed)[:, 1]\n",
    "            final_predictions.append(model_preds)\n",
    "        \n",
    "        # Average predictions from all models\n",
    "        return np.mean(final_predictions, axis=0)\n",
    "\n",
    "def main():\n",
    "    # Initialize data loader and model\n",
    "    data_loader = DataLoader()\n",
    "    X_train, X_test, y_train, submission = data_loader.load_data()\n",
    "    \n",
    "    # Train model\n",
    "    model = LoanApprovalModel(n_splits=3, random_state=42)\n",
    "    predictions = model.train_and_evaluate(X_train, y_train)\n",
    "    \n",
    "    # Analyze feature importance\n",
    "    analyzer = FeatureImportanceAnalyzer()\n",
    "    importance_summary = analyzer.get_feature_importance_summary(model.feature_importance)\n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    print(importance_summary.head(10))\n",
    "    \n",
    "    # Plot feature importance\n",
    "    fig = analyzer.plot_feature_importance(model.feature_importance)\n",
    "    plt.savefig('feature_importance.png', bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Generate test predictions\n",
    "    test_predictions = model.predict(X_test)\n",
    "    \n",
    "    # Create submission\n",
    "    submission['loan_status'] = test_predictions\n",
    "    submission.to_csv('ensemble_submission.csv')\n",
    "    print(\"\\nSubmission file created: ensemble_submission.csv\")\n",
    "    return submission, importance_summary\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    submission, importance_summary = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kaggle_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
